{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "75e62254",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pymorphy2\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from pymystem3 import Mystem\n",
    "\n",
    "from Levenshtein import distance as lev\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "from notebook.notebookapp import NotebookApp\n",
    "\n",
    "NotebookApp.iopub_data_rate_limit=100000000.0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Smol playground"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "как насчет небольшой стемминг\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mystem = Mystem()\n",
    "text = 'Как насчёт небольшого стемминга'\n",
    "lemmas = mystem.lemmatize(text)\n",
    "print(''.join(lemmas))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "convert = {\n",
    "    'A': 'ADJ',\n",
    "    'ADV': 'ADV',\n",
    "    'ADVPRO': 'ADV',\n",
    "    'ANUM': 'ADJ',\n",
    "    'APRO': 'DET',\n",
    "    'COM': 'ADJ',\n",
    "    'CONJ': 'SCONJ',\n",
    "    'INTJ': 'INTJ',\n",
    "    'NONLEX': 'X',\n",
    "    'NUM': 'NUM',\n",
    "    'PART': 'PART',\n",
    "    'PR': 'ADP',\n",
    "    'S': 'NOUN',\n",
    "    'SPRO': 'PRON',\n",
    "    'UNKN': 'X',\n",
    "    'V': 'VERB'\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "data": {
      "text/plain": "('какой', 'DET')"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tag(word='пожар'):\n",
    "    from pymystem3 import Mystem\n",
    "    stemmer = Mystem()\n",
    "    processed = stemmer.analyze(word)[0]\n",
    "    lemma = processed[\"analysis\"][0][\"lex\"].lower().strip()\n",
    "    pos = processed[\"analysis\"][0][\"gr\"].split(',')[0]\n",
    "    pos = pos.split('=')[0].strip()\n",
    "    pos = convert[pos]\n",
    "    tagged = lemma + '_' + pos\n",
    "    return lemma, pos\n",
    "tag('какой')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/art-\n",
      "[nltk_data]     bash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "info = api.info()  # show info about available models/datasets\n",
    "model = api.load(\"word2vec-ruscorpora-300\")\n",
    "\n",
    "nltk.download('stopwords')\n",
    "# pymorphy2 анализатор\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# стоп слова из nltk\n",
    "stops = nltk.corpus.stopwords.words('russian')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "<cyfunction levenshtein_distance at 0x7f8502fe52b0>"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "baddest_words = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "([[0, 'наличие_NOUN'],\n  [2, 'величие_NOUN'],\n  [2, 'галичий_ADJ'],\n  [2, 'галичин_NOUN'],\n  [2, 'куличие_NOUN'],\n  [2, 'наитие_NOUN'],\n  [2, 'налимий_ADJ'],\n  [2, 'налич_NOUN'],\n  [2, 'наличка_NOUN'],\n  [2, 'наличник_NOUN']],\n 'наличие',\n 'NOUN')"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entire_vocab = model.key_to_index\n",
    "\n",
    "def get_top_closest_in_vocab_and_tag(word):\n",
    "    results = [] # list of (dist, word)\n",
    "    try:\n",
    "        word = ''.join([c for c in word if c.isalpha()])\n",
    "        n_word, pos = tag(word)\n",
    "    except:\n",
    "        baddest_words.append(word)\n",
    "        return [[100000, ':(']], word, ''\n",
    "    for c in entire_vocab:\n",
    "        key, key_pos = c.split('_')\n",
    "        dist = lev(key, n_word)\n",
    "        results.append([dist, c])\n",
    "    return sorted(results)[:10], n_word, pos\n",
    "\n",
    "\n",
    "get_top_closest_in_vocab_and_tag('наличие')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31772 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ассалам\n",
      "алейкума\n",
      "\n",
      "заказать\n",
      "чёрный\n",
      "долго\n",
      "ждать\n",
      "\n",
      "цвет\n",
      "\n",
      "синий\n",
      "ми\n",
      "\n",
      "сие\n",
      "сегодня\n",
      "\n",
      "смотреть\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "сколько\n",
      "\n",
      "гб\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "сколько\n",
      "гб\n",
      "\n",
      "дело\n",
      "реза\n",
      "хийл\n",
      "понять\n",
      "\n",
      "\n",
      "\n",
      "чёрный\n",
      "цвет\n",
      "\n",
      "вариант\n",
      "мочь\n",
      "дать\n",
      "знать\n",
      "\n",
      "\n",
      "хотеть\n",
      "\n",
      "всё\n",
      "баркал\n",
      "\n",
      "\n",
      "\n",
      "гб\n",
      "ассалам\n",
      "алейкума\n",
      "цена\n",
      "ала\n",
      "брат\n",
      "\n",
      "\n",
      "\n",
      "скорый\n",
      "время\n",
      "баракалла\n",
      "\n",
      "розовый\n",
      "цвет\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "наличие\n",
      "\n",
      "мм\n",
      "\n",
      "\n",
      "\n",
      "скидка\n",
      "мочь\n",
      "предложить\n",
      "брат\n",
      "мочь\n",
      "\n",
      "\n",
      "смочь\n",
      "отдать\n",
      "гулкх\n",
      "дийра\n",
      "дар\n",
      "ахь\n",
      "\n",
      "\n",
      "суббота\n",
      "забрать\n",
      "сестра\n",
      "брат\n",
      "ассалам\n",
      "алейкума\n",
      "скинуть\n",
      "\n",
      "реквизит\n",
      "\n",
      "перевести\n",
      "деньга\n",
      "\n",
      "часы\n",
      "баракалла\n",
      "перевести\n",
      "прийти\n",
      "сестрёнка\n",
      "пусть\n",
      "забрать\n",
      "розовый\n",
      "дать\n",
      "беркат\n",
      "дойла\n",
      "массарна\n",
      "хийл\n",
      "\n",
      "мм\n",
      "ассалам\n",
      "алейкума\n",
      "брат\n",
      "кхи\n",
      "цхь\n",
      "часыш\n",
      "дезш\n",
      "хьать\n",
      "ду\n",
      "\n",
      "\n",
      "наличие\n",
      "чёрный\n",
      "\n",
      "\n",
      "цена\n",
      "отдать\n",
      "завтра\n",
      "забрать\n",
      "отложить\n",
      "\n",
      "\n",
      "вариант\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31772 [01:44<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../dat/glue_all_but_owner.txt', 'r') as fl:\n",
    "    messages = fl.readlines()\n",
    "    messages = [m.replace('\\n', '') for m in messages]\n",
    "\n",
    "rs = []\n",
    "\n",
    "def normalise_text(txt):\n",
    "    words = [''.join(c for c in x if c.isalpha()) for x in txt.split()]\n",
    "    words = [x for x in [morph.normal_forms(word)[0] for word in words ]\\\n",
    "            if x not in stops]\n",
    "    return ' '.join(words)\n",
    "\n",
    "for msg in tqdm.tqdm(messages):\n",
    "    words = [normalise_text(x) for x in msg.split()]\n",
    "    for word in words:\n",
    "        print(word)\n",
    "        res = get_top_closest_in_vocab_and_tag(word)\n",
    "        if res[0][0][0] > 0:\n",
    "            rs.append(res[0])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[1, 'асса_NOUN'],\n  [1, 'ассад_NOUN'],\n  [1, 'ассам_NOUN'],\n  [1, 'вассал_NOUN'],\n  [2, 'авал_NOUN'],\n  [2, 'аврал_NOUN'],\n  [2, 'айхал_NOUN'],\n  [2, 'аксай_NOUN'],\n  [2, 'амбал_NOUN'],\n  [2, 'анал_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'хилл_NOUN'],\n  [1, 'хойл_NOUN'],\n  [2, 'аил_NOUN'],\n  [2, 'ахилл_NOUN'],\n  [2, 'ахиол_NOUN'],\n  [2, 'бигл_NOUN'],\n  [2, 'бий_NOUN'],\n  [2, 'билл_NOUN'],\n  [2, 'биол_NOUN'],\n  [2, 'бойл_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'вге_NOUN'],\n  [1, 'ве_NOUN'],\n  [1, 'вее_NOUN'],\n  [1, 'вже_NOUN'],\n  [1, 'вне_ADP'],\n  [1, 'вне_ADV'],\n  [1, 'вре_NOUN'],\n  [1, 'вс_NOUN'],\n  [1, 'всв_NOUN'],\n  [1, 'всд_NOUN']],\n [[2, 'балаивать_VERB'],\n  [2, 'банковать_VERB'],\n  [2, 'бачивать_VERB'],\n  [2, 'башливать_VERB'],\n  [2, 'бургивать_VERB'],\n  [2, 'закивать_VERB'],\n  [2, 'каривать_VERB'],\n  [2, 'обшаркивать_VERB'],\n  [2, 'парковать_VERB'],\n  [2, 'убаюкивать_VERB']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'асса_NOUN'],\n  [1, 'ассад_NOUN'],\n  [1, 'ассам_NOUN'],\n  [1, 'вассал_NOUN'],\n  [2, 'авал_NOUN'],\n  [2, 'аврал_NOUN'],\n  [2, 'айхал_NOUN'],\n  [2, 'аксай_NOUN'],\n  [2, 'амбал_NOUN'],\n  [2, 'анал_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'каракалла_NOUN'],\n  [3, 'арабелла_NOUN'],\n  [3, 'барабаша_NOUN'],\n  [3, 'барабашка_NOUN'],\n  [3, 'барахолка_NOUN'],\n  [3, 'барбала_NOUN'],\n  [3, 'баркалов_NOUN'],\n  [3, 'баркарола_NOUN'],\n  [3, 'баркаролла_NOUN'],\n  [3, 'каравелла_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'гулко_ADV'],\n  [2, 'булах_NOUN'],\n  [2, 'булка_NOUN'],\n  [2, 'галах_NOUN'],\n  [2, 'галка_NOUN'],\n  [2, 'глах_NOUN'],\n  [2, 'гракх_NOUN'],\n  [2, 'гуак_NOUN'],\n  [2, 'губка_NOUN'],\n  [2, 'гузка_NOUN']],\n [[1, 'дейра_NOUN'],\n  [1, 'дибра_NOUN'],\n  [1, 'диора_NOUN'],\n  [1, 'дира_NOUN'],\n  [2, 'айра_NOUN'],\n  [2, 'бира_NOUN'],\n  [2, 'вира_INTJ'],\n  [2, 'вира_NOUN'],\n  [2, 'гидра_NOUN'],\n  [2, 'гимра_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'асса_NOUN'],\n  [1, 'ассад_NOUN'],\n  [1, 'ассам_NOUN'],\n  [1, 'вассал_NOUN'],\n  [2, 'авал_NOUN'],\n  [2, 'аврал_NOUN'],\n  [2, 'айхал_NOUN'],\n  [2, 'аксай_NOUN'],\n  [2, 'амбал_NOUN'],\n  [2, 'анал_NOUN']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[1, 'каракалла_NOUN'],\n  [3, 'арабелла_NOUN'],\n  [3, 'барабаша_NOUN'],\n  [3, 'барабашка_NOUN'],\n  [3, 'барахолка_NOUN'],\n  [3, 'барбала_NOUN'],\n  [3, 'баркалов_NOUN'],\n  [3, 'баркарола_NOUN'],\n  [3, 'баркаролла_NOUN'],\n  [3, 'каравелла_NOUN']],\n [[1, 'берат_NOUN'],\n  [1, 'берка_NOUN'],\n  [1, 'беркут_NOUN'],\n  [2, 'аркат_NOUN'],\n  [2, 'барка_NOUN'],\n  [2, 'барказ_ADV'],\n  [2, 'барказ_NOUN'],\n  [2, 'баркан_NOUN'],\n  [2, 'баркас_NOUN'],\n  [2, 'баркет_NOUN']],\n [[1, 'массажный_ADJ'],\n  [2, 'мансардный_ADJ'],\n  [2, 'массивный_ADJ'],\n  [2, 'пассадный_ADJ'],\n  [2, 'пассажный_ADJ'],\n  [2, 'пассатный_ADJ'],\n  [2, 'рассадный_ADJ'],\n  [3, 'аграрный_ADJ'],\n  [3, 'азарный_ADJ'],\n  [3, 'алтарный_ADJ']],\n [[1, 'хилл_NOUN'],\n  [1, 'хойл_NOUN'],\n  [2, 'аил_NOUN'],\n  [2, 'ахилл_NOUN'],\n  [2, 'ахиол_NOUN'],\n  [2, 'бигл_NOUN'],\n  [2, 'бий_NOUN'],\n  [2, 'билл_NOUN'],\n  [2, 'биол_NOUN'],\n  [2, 'бойл_NOUN']],\n [[100000, ':(']],\n [[1, 'асса_NOUN'],\n  [1, 'ассад_NOUN'],\n  [1, 'ассам_NOUN'],\n  [1, 'вассал_NOUN'],\n  [2, 'авал_NOUN'],\n  [2, 'аврал_NOUN'],\n  [2, 'айхал_NOUN'],\n  [2, 'аксай_NOUN'],\n  [2, 'амбал_NOUN'],\n  [2, 'анал_NOUN']],\n [[100000, ':(']],\n [[1, 'чарыш_NOUN'],\n  [1, 'часы_NOUN'],\n  [2, 'асык_NOUN'],\n  [2, 'барыш_NOUN'],\n  [2, 'засыл_NOUN'],\n  [2, 'камыш_NOUN'],\n  [2, 'капыш_NOUN'],\n  [2, 'касым_NOUN'],\n  [2, 'катыш_NOUN'],\n  [2, 'латыш_NOUN']],\n [[100000, ':(']],\n [[1, 'хать_NOUN'],\n  [1, 'хвать_INTJ'],\n  [2, 'авать_VERB'],\n  [2, 'акать_VERB'],\n  [2, 'ать_NOUN'],\n  [2, 'ахать_VERB'],\n  [2, 'бать_NOUN'],\n  [2, 'брать_VERB'],\n  [2, 'вать_NOUN'],\n  [2, 'врать_VERB']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']],\n [[100000, ':(']]]"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b598db02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#raw_data = pd.read_csv('doroga_v_les.csv', error_bad_lines=False).fillna('')\n",
    "#raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5b85ee2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data = pd.DataFrame(raw_data.apply(''.join, axis=1))\n",
    "#data.columns = ['text']\n",
    "#data.to_csv('doroga_v_les.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "158e3ab3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../dat/glue_split_owner.txt', sep=';', header=None)\n",
    "data.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d58175b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/art-bash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "sent_tokenizer = lambda sent: nltk.sent_tokenize(sent, language=\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d431577a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/art-\n",
      "[nltk_data]     bash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "# pymorphy2 анализатор\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "# стоп слова из nltk\n",
    "stops = nltk.corpus.stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9ef0d92",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def proccess_text(text):\n",
    "    text = text.lower()\n",
    "    sents = sent_tokenizer(text)\n",
    "    words = list(\n",
    "        itertools.chain.from_iterable(\n",
    "            word_tokenizer.tokenize_sents(sents)))\n",
    "    return [x for x in [morph.normal_forms(word)[0] for word in words ]\\\n",
    "            if x not in stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5b12e68",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 20307/63860 [00:33<01:12, 602.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [49], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m data_text \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(data))):\n\u001B[0;32m----> 6\u001B[0m      data_text\u001B[38;5;241m.\u001B[39miloc[i] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(proccess_text(data_text\u001B[38;5;241m.\u001B[39miloc[i]))\n\u001B[1;32m      8\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data_text\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#data_text.columns = ['text']\u001B[39;00m\n",
      "Cell \u001B[0;32mIn [48], line 7\u001B[0m, in \u001B[0;36mproccess_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      3\u001B[0m sents \u001B[38;5;241m=\u001B[39m sent_tokenizer(text)\n\u001B[1;32m      4\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m      5\u001B[0m     itertools\u001B[38;5;241m.\u001B[39mchain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m      6\u001B[0m         word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize_sents(sents)))\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [morph\u001B[38;5;241m.\u001B[39mnormal_forms(word)[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words ]\\\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stops]\n",
      "Cell \u001B[0;32mIn [48], line 7\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      3\u001B[0m sents \u001B[38;5;241m=\u001B[39m sent_tokenizer(text)\n\u001B[1;32m      4\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m      5\u001B[0m     itertools\u001B[38;5;241m.\u001B[39mchain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m      6\u001B[0m         word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize_sents(sents)))\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [x \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m [\u001B[43mmorph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnormal_forms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m words ]\\\n\u001B[1;32m      8\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m x \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stops]\n",
      "File \u001B[0;32m~/venvs/ai-secretary/lib/python3.8/site-packages/pymorphy2/analyzer.py:350\u001B[0m, in \u001B[0;36mMorphAnalyzer.normal_forms\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m    347\u001B[0m seen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[1;32m    348\u001B[0m result \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m--> 350\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparse\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    351\u001B[0m     normal_form \u001B[38;5;241m=\u001B[39m p[\u001B[38;5;241m2\u001B[39m]\n\u001B[1;32m    352\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m normal_form \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m seen:\n",
      "File \u001B[0;32m~/venvs/ai-secretary/lib/python3.8/site-packages/pymorphy2/analyzer.py:321\u001B[0m, in \u001B[0;36mMorphAnalyzer.parse\u001B[0;34m(self, word)\u001B[0m\n\u001B[1;32m    318\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    320\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprob_estimator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 321\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprob_estimator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_to_parses\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_lower\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mres\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_result_type \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    324\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/venvs/ai-secretary/lib/python3.8/site-packages/pymorphy2/analyzer.py:77\u001B[0m, in \u001B[0;36mProbabilityEstimator.apply_to_parses\u001B[0;34m(self, word, word_lower, parses)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parses:\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parses\n\u001B[0;32m---> 77\u001B[0m probs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mp_t_given_w\u001B[38;5;241m.\u001B[39mprob(word_lower, tag)\n\u001B[1;32m     78\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m (word, tag, normal_form, score, methods_stack) \u001B[38;5;129;01min\u001B[39;00m parses]\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28msum\u001B[39m(probs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;66;03m# no P(t|w) information is available; return normalized estimate\u001B[39;00m\n\u001B[1;32m     82\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mmap\u001B[39m(_score_getter, parses))\n",
      "File \u001B[0;32m~/venvs/ai-secretary/lib/python3.8/site-packages/pymorphy2/analyzer.py:77\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m parses:\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parses\n\u001B[0;32m---> 77\u001B[0m probs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp_t_given_w\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprob\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword_lower\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtag\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     78\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m (word, tag, normal_form, score, methods_stack) \u001B[38;5;129;01min\u001B[39;00m parses]\n\u001B[1;32m     80\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28msum\u001B[39m(probs) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     81\u001B[0m     \u001B[38;5;66;03m# no P(t|w) information is available; return normalized estimate\u001B[39;00m\n\u001B[1;32m     82\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m \u001B[38;5;241m/\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;28mmap\u001B[39m(_score_getter, parses))\n",
      "File \u001B[0;32m~/venvs/ai-secretary/lib/python3.8/site-packages/pymorphy2/dawg.py:65\u001B[0m, in \u001B[0;36mConditionalProbDistDAWG.prob\u001B[0;34m(self, word, tag)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprob\u001B[39m(\u001B[38;5;28mself\u001B[39m, word, tag):\n\u001B[0;32m---> 65\u001B[0m     dawg_key \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (word, tag)\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget(dawg_key, \u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mMULTIPLIER\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'[a-zа-яёЁА-ЯA-Z]\\w+\\'?\\w*')\n",
    "\n",
    "data_text = data['text']\n",
    "\n",
    "for i in tqdm.tqdm(range(len(data))):\n",
    "     data_text.iloc[i] = ' '.join(proccess_text(data_text.iloc[i]))\n",
    "        \n",
    "data['text'] = data_text\n",
    "#data_text.columns = ['text']\n",
    "data.to_csv('doroga_v_les_proccessed.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91676e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data_text = pd.read_csv('doroga_v_les_proccessed.csv', sep=';')\n",
    "#data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc8213b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b1a59e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#data_text = pd.DataFrame(data_text.apply(replace('добрый', ''), axis=1))\n",
    "#data_text['text'] = data_text['text'].str.replace('добрый','')\n",
    "#data_text['text'] = data_text['text'].str.replace('день','')\n",
    "#data_text['text'] = data_text['text'].str.replace('здравствуйте','')\n",
    "#data_text['text'] = data_text['text'].str.replace('утро','')\n",
    "#data_text['text'] = data_text['text'].str.replace('спасибо','')\n",
    "\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d87141",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zа-яёЁА-ЯA-Z]\\w+\\'?\\w*')\n",
    "data['tokenized_text'] = data['text'].apply(lambda row: tokenizer.tokenize(row))\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea714fd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb907418",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data['tokenized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0626da94",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Filter greetings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283d9eca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_most_freq_words(str, n=None):\n",
    "    vect = CountVectorizer().fit(str)\n",
    "    bag_of_words = vect.transform(str)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    freq = [(word, sum_words[0, idx]) for word, idx in vect.vocabulary_.items()]\n",
    "    freq =sorted(freq, key = lambda x: x[1], reverse=True)\n",
    "    return freq[:n]\n",
    "\n",
    "get_most_freq_words([word for messege in data.tokenized_text for word in messege],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcbddf",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "messeges_dictionary = Dictionary(data.tokenized_text)\n",
    "\n",
    "messeges_corpus = [messeges_dictionary.doc2bow(messege) for messege in data.tokenized_text]\n",
    "\n",
    "# compute coherence\n",
    "messeges_coherence = []\n",
    "for nb_topics in tqdm.tqdm(range(1,36)):\n",
    "    lda = LdaModel(messeges_corpus, num_topics = nb_topics, id2word = messeges_dictionary, passes=10)\n",
    "    cohm = CoherenceModel(model=lda, corpus=messeges_corpus, dictionary=messeges_dictionary, coherence='u_mass')\n",
    "    coh = cohm.get_coherence()\n",
    "    messeges_coherence.append(coh)\n",
    "\n",
    "# visualize coherence\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1,36),messeges_coherence)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bbf63f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "k = 30 # number of topics\n",
    "\n",
    "messeges_lda = LdaModel(messeges_corpus, num_topics = k, id2word = messeges_dictionary, passes=10)\n",
    "\n",
    "def plot_top_words(lda=messeges_lda, nb_topics=k, nb_words=15):\n",
    "    top_words = [[word for word,_ in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n",
    "    top_betas = [[beta for _,beta in lda.show_topic(topic_id, topn=50)] for topic_id in range(lda.num_topics)]\n",
    "\n",
    "    gs  = gridspec.GridSpec(round(math.sqrt(k))+1,round(math.sqrt(k))+1)\n",
    "    gs.update(wspace=0.5, hspace=0.5)\n",
    "    plt.figure(figsize=(20,15))\n",
    "    for i in range(nb_topics):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.barh(range(nb_words), top_betas[i][:nb_words], align='center',color='blue', ecolor='black')\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_yticks(range(nb_words))\n",
    "        ax.set_yticklabels(top_words[i][:nb_words])\n",
    "        plt.title(\"Topic \"+str(i))\n",
    "        \n",
    "plot_top_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af062c7f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Hrabovoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839f962",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afedf9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "texts = data['text'].values.tolist()\n",
    "topics = data['tag'].values.tolist()\n",
    "\n",
    "texts, topics = shuffle(texts, topics)\n",
    "\n",
    "texts_train = texts[:45]\n",
    "texts_test = texts[45:]\n",
    "\n",
    "topics_train = topics[:45]\n",
    "topics_test = topics[45:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e96e20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "vectorizer.fit(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdd6e11",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_doc_vectors = vectorizer.transform(texts_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a39de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "lda.fit(train_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57aed66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_train = lda.transform(train_doc_vectors)\n",
    "topics_train = np.array(topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748b3dd4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=10)\n",
    "classifier.fit(feature_train, topics_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb9893",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_doc_vectors = vectorizer.transform(texts_test)\n",
    "feature_test = lda.transform(test_doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea610bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred = classifier.predict(feature_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca477ca8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics_test = np.array(topics_test)\n",
    "topics_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(classification_report(topics_test, pred, zero_division=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}